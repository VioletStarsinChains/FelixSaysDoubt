KC: Which philosopher first wrote about ontology?

CG: The first philosopher generally credited with writing about ontologyâ€”that is, the systematic study of being as beingâ€”is Aristotle.
	â€¢	The actual word ontology (from Greek ontos = being, logos = discourse) didnâ€™t exist in Aristotleâ€™s time; it appears much later, in the 17th century (notably in works by Jacob Lorhard in 1606 and Rudolf GÃ¶ckel in 1613).
	â€¢	But the concept is traced back to Aristotleâ€™s Metaphysics, where he distinguishes between different senses of being and tries to determine the fundamental principles underlying all that exists. He called this inquiry the search for being qua being (to on hÃª on).

That said, Parmenides (5th century BCE) is often regarded as the first philosopher to make â€œbeingâ€ itself the explicit subject of thought. His poem On Nature argues that â€œwhat is, isâ€ and that non-being cannot be thought or spoken of. Plato, too, in dialogues like the Sophist and Parmenides, wrestled with the nature of being and non-being.

So:
	â€¢	Parmenides was the first to write directly about being.
	â€¢	Plato deepened the discussion.
	â€¢	Aristotle systematized it in a way that anticipates what we now call ontology.
	â€¢	The term â€œontologyâ€ itself was coined in the early 1600s.

Would you like me to map out the timeline of ontologyâ€™s development (Parmenides â†’ Plato â†’ Aristotle â†’ Scholastics â†’ 17th century term â€œontologyâ€), or are you mainly interested in who first framed the question?

KC: So why did usage of the term start climbing rapidly in the 1990s and peak around 2011? (screenshot of Ngram chart included in prompt)

CG: Great question â€” and the Ngram chart youâ€™ve pulled up is really telling. Letâ€™s unpack both parts:

1. Why the rise in the 1990sâ€“2000s?
The sharp increase in the use of the word ontology wasnâ€™t driven only by philosophers. It was driven heavily by computer science and information systems:
	â€¢	In artificial intelligence (AI) and knowledge representation, â€œontologyâ€ became the term for structured vocabularies that formally describe entities and their relationships (e.g., medical ontologies, semantic web, bioinformatics).
	â€¢	The Semantic Web initiative, championed by Tim Berners-Lee in the late 1990s, popularized ontologies as a foundation for machine-readable knowledge.
	â€¢	Fields like bioinformatics, library science, linguistics, and software engineering all adopted the term.

So, starting in the early 1990s, â€œontologyâ€ jumped from philosophy into technical and applied sciences, massively multiplying its usage.

2. What happened around 2011?
The peak in 2011 likely reflects the height of the Semantic Web hype cycle. Around that time:
	â€¢	Linked Data and Semantic Web technologies were at their most widely discussed in both academic and technical literature.
	â€¢	Projects like DBpedia, FOAF, OWL (Web Ontology Language), and schema.org were being developed and adopted.
	â€¢	There was also a surge of ontology-related publications in biomedical research, especially the Gene Ontology project, which by then had become a standard in genomics and molecular biology.

After about 2011, the termâ€™s usage dipped slightly, possibly because â€œontologyâ€ as a buzzword stabilized into established practice. The hype cooled, even though the work continues in many fields.

â¸»

ğŸ‘‰ So in short:
	â€¢	Philosophy gave us the word.
	â€¢	AI, computer science, and bioinformatics made it explode in the 1990sâ€“2010s.
	â€¢	2011 marks the peak of Semantic Web + bio-ontology excitement.

Would you like me to make a side-by-side timeline (philosophy vs. computer science) to show how the wordâ€™s meaning and usage evolved?

KC: I'm guessing ontology is tied more closely to epistemology in philosophy than in computer science?

CG: Exactly â€” youâ€™re on point.

In philosophy:
	â€¢	Ontology and epistemology are like twin pillars:
	â€¢	Ontology = what exists (the structure of reality).
	â€¢	Epistemology = how we know (the structure of knowledge).
	â€¢	Theyâ€™re tightly linked because oneâ€™s account of being often constrains oneâ€™s account of knowing. For example:
	â€¢	Plato: ontology of eternal Forms â†” epistemology of recollection/ rational intuition.
	â€¢	Kant: ontology of phenomena/noumena â†” epistemology of categories/conditions of possible experience.
	â€¢	Heidegger: ontology of Dasein â†” epistemology rooted in existential situatedness.

So, in philosophy, to talk ontology almost always raises epistemological issues: â€œWhat is real?â€ runs into â€œHow do we know itâ€™s real?â€

â¸»

In computer science:
	â€¢	The word ontology is more technical and pragmatic. It usually means a formal specification of a conceptualization (Tom Gruberâ€™s famous definition, 1993).
	â€¢	An ontology in CS is like a structured schema: it defines entities, classes, relationships, and rules so machines can share data meaningfully.
	â€¢	It doesnâ€™t care about epistemology in the classical sense (justification, belief, truth). Instead, it assumes knowledge can be represented in machine-readable form if you design the right ontology.
	â€¢	The philosophical worries about â€œbut how do we know this model corresponds to reality?â€ are usually bracketed off.

â¸»

So:
	â€¢	In philosophy, ontology and epistemology are inseparable.
	â€¢	In computer science, ontology is mostly severed from epistemology â€” itâ€™s about shared vocabularies for interoperability, not the nature of knowing.

ğŸ‘‰ The irony is that the CS usage secretly inherits those old philosophical tensions, but engineers usually treat them as implementation problems, not metaphysical ones.

Would you like me to sketch how computer ontologies could actually reintroduce epistemological problems (e.g. bias, incompleteness, authority over categories) â€” which is where things get politically spicy?

KC: Yes please. 

CG: Perfect â€” this is where it gets juicy. Letâ€™s trace how epistemology sneaks back into computer ontologies:

â¸»

1. Choice of Categories = Epistemic Authority
	â€¢	In philosophy: â€œWhat kinds of things exist?â€
	â€¢	In CS: â€œWhat classes and relations do we include in the ontology?â€
	â€¢	Problem: Whoever designs the ontology decides whatâ€™s â€œreal enoughâ€ to encode. For example:
	â€¢	A medical ontology may encode â€œschizophreniaâ€ but not â€œC-PTSD,â€ shaping research and treatment.
	â€¢	A social ontology may encode â€œgender: male/femaleâ€ but exclude non-binary identities.

This mirrors philosophical worries about epistemic injustice (Miranda Fricker): what is excluded from a conceptual scheme canâ€™t be known, or even said.

â¸»

2. Epistemic Bias Hidden in Formalism
	â€¢	Ontologies in CS are formal: classes, predicates, hierarchies.
	â€¢	This gives the appearance of objectivity. But the design reflects cultural/disciplinary assumptions.
	â€¢	Example: The Gene Ontology assumes hierarchical, tree-like relationships. But not all biological processes are tree-structured â€” some are networked, messy, recursive.

So, an ontology encodes not just facts, but a worldview about how facts hang together.

â¸»

3. Interoperability = Epistemic Translation Problems
	â€¢	Computer ontologies are often built to integrate data across systems.
	â€¢	But translation always involves loss or distortion: what one system encodes may not map cleanly to another.
	â€¢	Philosophical echo: Kuhnâ€™s incommensurability of paradigms â€” different conceptual schemes canâ€™t always be reconciled.

So, when databases â€œalign ontologies,â€ theyâ€™re performing a technical fix for an epistemological problem.

â¸»

4. Power & Surveillance = Who Gets to Define Reality?
	â€¢	Palantir, government agencies, Big Tech firms build ontologies to track people, events, risks.
	â€¢	Their ontologies shape what counts as suspicious behavior, a terrorist, a patient, a consumer.
	â€¢	Ontology here is not just academic â€” itâ€™s political epistemology: whoever controls the categories controls what the system knows.

This is what makes behavioral futures markets so insidious: the ontologies that define â€œanomaly,â€ â€œcompliance,â€ or â€œriskâ€ are built into the machinery of prediction, but they reflect political/economic agendas.

â¸»

5. Epistemic Fragility of Automation
	â€¢	Once an ontology is encoded, the system â€œknowsâ€ only what the ontology permits.
	â€¢	This creates epistemic blind spots: if reality changes, or new categories emerge, the system canâ€™t register them.
	â€¢	Example: Early AI ontologies in the 1990s couldnâ€™t handle irony, memes, or emoji. These gaps got exploited by users (think 4chanâ€™s coded language slipping past filters).

Philosophically: this is the problem of induction and the limits of formalization â€” no ontology can capture reality exhaustively.

â¸»

âœ… So in short:
Computer ontologies look like they cut ontology off from epistemology, but in practice they reinstantiate epistemological problems as design choices, translation issues, blind spots, and ultimately power struggles over who gets to define reality for machines.
